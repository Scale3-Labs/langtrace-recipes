{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending Traces from Langtrace to New Relic: A Step-by-Step Guide\n",
    "\n",
    "This notebook demonstrates how to use Langtrace with OpenAI and Qdrant, and send traces to New Relic using OpenTelemetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "from qdrant_client import QdrantClient\n",
    "from langtrace_python_sdk import langtrace, with_langtrace_root_span\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize environment and clients\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "langtrace.init(api_key='YOUR_LANGTRACE_API_KEY')\n",
    "qdrant_client = QdrantClient(\":memory:\") \n",
    "openai_client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"Libraries imported and clients initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Key Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_langtrace_root_span(\"initialize_knowledge_base\")\n",
    "def initialize_knowledge_base(documents: List[str]) -> None:\n",
    "    start_time = time.time()\n",
    "    qdrant_client.add(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        documents=documents\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Knowledge base initialized with {len(documents)} documents in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Knowledge base initialization function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Query Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_langtrace_root_span(\"query_vector_db\")\n",
    "def query_vector_db(question: str, n_points: int = 3) -> List[Dict[str, Any]]:\n",
    "    start_time = time.time()\n",
    "    results = qdrant_client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Vector DB queried in {end_time - start_time:.2f} seconds, returned {len(results)} results\")\n",
    "    return results\n",
    "\n",
    "print(\"Vector database query function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_langtrace_root_span(\"generate_llm_response\")\n",
    "def generate_llm_response(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    start_time = time.time()\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        timeout=10.0,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    response = completion.choices[0].message.content\n",
    "    print(f\"LLM response generated in {end_time - start_time:.2f} seconds\")\n",
    "    return response\n",
    "\n",
    "print(\"LLM response generation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Implement RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_langtrace_root_span(\"rag\")\n",
    "def rag(question: str, n_points: int = 3) -> str:\n",
    "    print(f\"Processing RAG for question: {question}\")\n",
    "    \n",
    "    context_start = time.time()\n",
    "    context = \"\\n\".join([r.document for r in query_vector_db(question, n_points)])\n",
    "    context_end = time.time()\n",
    "    print(f\"Context retrieved in {context_end - context_start:.2f} seconds\")\n",
    "    \n",
    "    prompt_start = time.time()\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a software architect.\n",
    "    Answer the following question using the provided context.\n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "\n",
    "    Question: {question.strip()}\n",
    "\n",
    "    Context:\n",
    "    {context.strip()}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt_end = time.time()\n",
    "    print(f\"Prompt constructed in {prompt_end - prompt_start:.2f} seconds\")\n",
    "    \n",
    "    answer = generate_llm_response(metaprompt)\n",
    "    print(f\"RAG completed, answer length: {len(answer)} characters\")\n",
    "    return answer\n",
    "\n",
    "print(\"RAG function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate Different Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_different_queries():\n",
    "    questions = [\n",
    "        \"What is Qdrant used for?\",\n",
    "        \"How does Docker help developers?\",\n",
    "        \"What is the purpose of MySQL?\",\n",
    "        \"Can you explain what FastAPI is?\",\n",
    "    ]\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = rag(question)\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {str(e)}\\n\")\n",
    "\n",
    "print(\"Query demonstration function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Knowledge Base and Run Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge base\n",
    "documents = [\n",
    "    \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
    "    \"Docker helps developers build, share, and run applications anywhere â€” without tedious environment configuration or management.\",\n",
    "    \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
    "    \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
    "    \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
    "    \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
    "    \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
    "    \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
    "]\n",
    "initialize_knowledge_base(documents)\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_different_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Traces\n",
    "\n",
    "After running the queries, you can analyze the traces as follows:\n",
    "\n",
    "1. Check the Langtrace dashboard for a visual representation of the traces.\n",
    "2. Look for the 'rag' root span and its child spans to understand the flow.\n",
    "3. Examine the timing information printed for each operation to identify potential bottlenecks.\n",
    "4. Review any error messages printed to understand and address issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
