{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fashion Bot Jupyter Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates how to create a simple fashion recommendation bot using Langtrace for tracing and Ollama for local language model inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites:\n",
        "* Sign-up for a free account on [LangTrace](https://langtrace.ai/).\n",
        "* Create a project then generate an API key. Refer to quickstart documentation for help.\n",
        "* Ensure your laptop has at least 16GB of RAM for optimal performance with Ollama.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0: Set up Ollama Locally\n",
        "\n",
        "Before we start coding, we need to set up Ollama on your local machine:\n",
        "\n",
        "1. Download and install Ollama from [https://ollama.com/download/Ollama-darwin.zip](https://ollama.com/download/Ollama-darwin.zip)\n",
        "2. Open a terminal window and run the following command to download and run the Llama3 model locally:\n",
        "\n",
        "   ```bash\n",
        "   ollama run llama3\n",
        "   ```\n",
        "\n",
        "   This command is similar to Docker commands. It will pull the Llama3 model and run it locally.\n",
        "\n",
        "   ![Ollama Terminal Output](../../assets/ollamainstall.png)\n",
        "\n",
        "3. Once the model is downloaded and running, you should see a terminal prompt where you can start chatting.\n",
        "\n",
        "Note: For further customization and to use `Modelfile` to create your own custom system prompt, refer to the [Ollama documentation](https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md).\n",
        "\n",
        "Now that Ollama is set up locally, we can proceed with our Fashion Bot implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install & Initialize Langtrace\n",
        "First, we need to install and import the neccessary libraries then initialize Langtrace with our API key. In a production environment, you should use environment variables or a secure method to store your API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install ollama langtrace-python-sdk "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langtrace_python_sdk import langtrace, with_langtrace_root_span\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize langtrace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace 'YOUR_API_KEY' with your actual API key\n",
        "langtrace.init(api_key='8032673cc3f998540b946a3c0cfae5eee7a3786694a043463e466b6fab242f05')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define the Recommendation Function\n",
        "We'll create a function that generates fashion recommendations based on the event type. This function will be wrapped with Langtrace's root span decorator for tracing. Be sure to update the model name to whatever model you're using from Ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@with_langtrace_root_span()\n",
        "def give_recs(event_type):\n",
        "    # Replace 'llama3' with the model you want to use\n",
        "    response = ollama.chat(model='llama3', messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f'You are an AI assistant with expertise in mens clothing. Help me pick clothing for a {event_type} at work. Dont ask any follow up questions.',\n",
        "        },\n",
        "    ])\n",
        "    return response['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Test the Function\n",
        "# \n",
        "Let's test our function with a sample event type to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running fashionista bot...\")\n",
        "recommendation = give_recs(\"black tie dinner\")\n",
        "print(recommendation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Interactive User Input\n",
        "Now, let's create an interactive cell where users can input their own event types and get customized recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "event_type = input(\"Enter the type of event (e.g., casual Friday, business meeting): \")\n",
        "custom_recommendation = give_recs(event_type)\n",
        "print(f\"\\nRecommendation for {event_type}:\")\n",
        "print(custom_recommendation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image info](../../assets/ollamatrace1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        " \n",
        "In this notebook, we've created a simple fashion recommendation bot using Langtrace for tracing and Ollama for local language model inference. This demonstrates how easy it is to integrate these tools into your AI applications.\n",
        " \n",
        "Some potential next steps:\n",
        "* Add more complex prompts or context to improve recommendations\n",
        "* Implement a feedback loop to improve recommendations over time\n",
        "* Analyze Langtrace data to optimize performance and identify bottlenecks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
